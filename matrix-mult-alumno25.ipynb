{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454c61ba-825d-4290-b7ed-135a775fe002",
   "metadata": {},
   "source": [
    "### Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8019c56a-e5ce-469b-a406-9c6056ab9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 s ± 6.16 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "Result shape: (7000, 7000)\n",
      "Result type: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Large matrices (adjust size as needed)\n",
    "n = 7000  # For very large matrices, ensure you have enough RAM\n",
    "A = np.random.rand(n, n).astype(np.float32)\n",
    "B = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "C = np.dot(A, B)  # warm-up and Matrix multiplication\n",
    "\n",
    "%timeit -r 2 -o np.dot(A, B)\n",
    "\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result type: {C.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a3812-2545-4626-a4e4-39668ece7cf6",
   "metadata": {},
   "source": [
    "## Multiplicación con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e220304-9bec-4755-94da-912417b3cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1080 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1080 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 ms ± 388 μs per loop (mean ± std. dev. of 2 runs, 1,000 loops each)\n",
      "Result shape: (7000, 7000)\n",
      "Result type: float32\n",
      "Max difference with NumPy: 1.21e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "A_torch = torch.from_numpy(A).to(device)\n",
    "B_torch = torch.from_numpy(B).to(device)\n",
    "\n",
    "# warm-up\n",
    "C_torch = torch.matmul(A_torch, B_torch)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "C_torch = torch.matmul(A_torch, B_torch)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Tiempo PyTorch:\", end - start)\n",
    "\n",
    "C_result = C_torch.cpu().numpy()\n",
    "print(C_result.shape)\n",
    "print(C_result.dtype)\n",
    "\n",
    "diff = np.max(np.abs(C - C_result))\n",
    "print(\"Diferencia máxima:\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80560d20-ab8d-4f17-b70e-6b9295c267e3",
   "metadata": {},
   "source": [
    "## Cálculo de π con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0be59f-0b4f-42f2-9c94-1d8cc8f5c33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m N = \u001b[32m5\u001b[39m * \u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m x = (\u001b[32m2\u001b[39m * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m - \u001b[32m1\u001b[39m).float()\n\u001b[32m      4\u001b[39m y = (\u001b[32m2\u001b[39m * torch.rand(N, device=device) - \u001b[32m1\u001b[39m).float()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(x.dtype)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def calc_pi_pytorch(N, device):\n",
    "    x = torch.rand(N, device=device) * 2 - 1\n",
    "    y = torch.rand(N, device=device) * 2 - 1\n",
    "    inside = (x * x + y * y) < 1.0\n",
    "    M = inside.float().sum().item()\n",
    "    return 4 * M / N\n",
    "\n",
    "sizes = [1000000, 10000000]\n",
    "\n",
    "for N in sizes:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    pi_value = calc_pi_pytorch(N, device)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"N =\", N)\n",
    "    print(\"pi =\", pi_value)\n",
    "    print(\"error =\", abs(pi_value - np.pi))\n",
    "    print(\"tiempo =\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c60067-9c85-4edb-9763-a0562da6e734",
   "metadata": {},
   "source": [
    "## NumPy (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2753cd-7fc6-42af-a81b-8cea8d78e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pi_numpy(N):\n",
    "    x = np.random.uniform(-1, 1, N)\n",
    "    y = np.random.uniform(-1, 1, N)\n",
    "    inside = (x * x + y * y) < 1.0\n",
    "    return 4 * np.sum(inside) / N\n",
    "\n",
    "N = 10000000\n",
    "\n",
    "start = time.time()\n",
    "pi_numpy = calc_pi_numpy(N)\n",
    "end = time.time()\n",
    "\n",
    "print(\"pi NumPy =\", pi_numpy)\n",
    "print(\"tiempo NumPy =\", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe95aa-4d69-43b1-8c24-bffe920e5013",
   "metadata": {},
   "source": [
    "## Comparación CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff935dd-c161-47f2-abfe-928d61b36f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "start = time.time()\n",
    "C_numpy = np.dot(A, B)\n",
    "end = time.time()\n",
    "time_cpu = end - start\n",
    "print(\"Tiempo NumPy:\", time_cpu)\n",
    "\n",
    "# PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    C_torch = torch.matmul(A_torch, B_torch)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    time_gpu = end - start\n",
    "    print(\"Tiempo PyTorch GPU:\", time_gpu)\n",
    "    print(\"Speedup:\", time_cpu / time_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c993020-8dc1-44d2-9e8d-6b128980c1ff",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "En la multiplicación de matrices de tamaño 7000 × 7000 se observa una diferencia clara entre la ejecución en CPU con NumPy y la ejecución en GPU con PyTorch. NumPy tarda aproximadamente 0.675 segundos por ejecución, mientras que PyTorch en GPU reduce el tiempo a unos 0.060 segundos. Esto supone una aceleración cercana a 11 veces, lo que demuestra que la GPU es mucho más eficiente para operaciones matriciales grandes debido a su alto paralelismo. La diferencia máxima entre los resultados de NumPy y PyTorch es pequeña y se explica por el uso de precisión float32 y por el distinto orden de las operaciones en GPU, por lo que el resultado puede considerarse correcto.\n",
    "\n",
    "En el cálculo de π mediante el método de Monte Carlo con PyTorch en GPU, para un millón de puntos el valor obtenido presenta un error relativamente mayor y un tiempo de ejecución más alto, debido principalmente a la sobrecarga inicial y a la generación de números aleatorios. Sin embargo, al aumentar el número de puntos a diez millones, el error disminuye notablemente y el tiempo de ejecución se reduce de forma drástica, mostrando que la GPU aprovecha mejor su paralelismo cuando el volumen de trabajo es grande.\n",
    "\n",
    "La comparación directa entre PyTorch en GPU y NumPy en CPU para el cálculo de π con diez millones de puntos confirma esta tendencia. NumPy tarda alrededor de 0.179 segundos, mientras que PyTorch en GPU completa el cálculo en apenas 0.0016 segundos, obteniendo una aceleración significativa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ad803-54ef-44d3-9ea1-b125d88d5e62",
   "metadata": {},
   "source": [
    "675 ms ± 1.78 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
    "\n",
    "Result shape: (7000, 7000)\n",
    "\n",
    "Result type: float32\n",
    "________________________________________________________________________\n",
    "cuda\n",
    "\n",
    "Tiempo PyTorch: 0.060648202896118164\n",
    "\n",
    "(7000, 7000)\n",
    "\n",
    "float32\n",
    "\n",
    "Diferencia máxima: 0.011962891\n",
    "________________________________________________________________________\n",
    "cuda\n",
    "\n",
    "N = 1000000\n",
    "\n",
    "pi = 3.144312\n",
    "\n",
    "error = 0.002719346410207102\n",
    "\n",
    "tiempo = 1.1935160160064697\n",
    "________________________________________________________________________\n",
    "N = 10000000\n",
    "\n",
    "pi = 3.1425156\n",
    "\n",
    "error = 0.0009229464102067375\n",
    "\n",
    "tiempo = 0.0016279220581054688\n",
    "\n",
    "pi NumPy = 3.1423724\n",
    "\n",
    "tiempo NumPy = 0.1786792278289795\n",
    "________________________________________________________________________\n",
    "Tiempo NumPy: 0.6612417697906494\n",
    "\n",
    "Tiempo PyTorch GPU: 0.060597896575927734\n",
    "\n",
    "Speedup: 10.91195911333538"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
