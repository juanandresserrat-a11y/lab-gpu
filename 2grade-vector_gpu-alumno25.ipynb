{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.65 ms ± 29.1 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "19.0 ms ± 25.7 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "19.1 ms ± 40.0 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "import time\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Función para simular %timeit\n",
    "def my_timeit(func, *args, n=5, r=2):\n",
    "    all_times = []\n",
    "    for _ in range(r):\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n):\n",
    "            result = func(*args)\n",
    "        end = time.perf_counter()\n",
    "        all_times.append((end - start) / n)\n",
    "    \n",
    "    mean_time = np.mean(all_times) * 1000  # convertir a ms\n",
    "    std_time = np.std(all_times) * 1000 * 1000  # convertir a μs\n",
    "    \n",
    "    # Formatear salida como %timeit\n",
    "    if mean_time < 1:\n",
    "        mean_str = f\"{mean_time*1000:.2f} μs\"\n",
    "        std_str = f\"{std_time:.1f} ns\"\n",
    "    elif mean_time < 10:\n",
    "        mean_str = f\"{mean_time:.2f} ms\"\n",
    "        std_str = f\"{std_time:.1f} μs\"\n",
    "    else:\n",
    "        mean_str = f\"{mean_time:.1f} ms\"\n",
    "        std_str = f\"{std_time:.1f} μs\"\n",
    "    \n",
    "    print(f\"{mean_str} ± {std_str} per loop (mean ± std. dev. of {r} runs, {n} loops each)\")\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "my_timeit(grade2_vector, a_cpu, b_cpu, a, b, c, n=5, r=2)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "my_timeit(grade2_ufunc, a_cpu, b_cpu, a, b, c, n=5, r=2)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "my_timeit(lambda: a*a_cpu**2 + b*b_cpu + c, n=5, r=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c3be-92a0-4c04-bf71-d78a11b1af38",
   "metadata": {},
   "source": [
    "## GPU con cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10682699-d56b-45a6-86f4-c31bbf1f2236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Time with copy:\n",
      "0.018155039978027344 ± 0.0001276826854245235\n",
      "\n",
      "Time without copy:\n",
      "0.002018892812728882 ± 3.054137354243286e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x*x + b*y + c\n",
    "\n",
    "size = 5_000_000\n",
    "\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10.0\n",
    "\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "def grade2_cupy_with_copy(x_cpu, y_cpu, a, b, c):\n",
    "    x_gpu = cp.asarray(x_cpu)\n",
    "    y_gpu = cp.asarray(y_cpu)\n",
    "    z_gpu = grade2_ufunc(x_gpu, y_gpu, a, b, c)\n",
    "    return cp.asnumpy(z_gpu)\n",
    "\n",
    "_ = grade2_cupy_with_copy(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "tiempo = benchmark(\n",
    "    grade2_cupy_with_copy,\n",
    "    (a_cpu, b_cpu, a, b, c),\n",
    "    n_repeat=5\n",
    ")\n",
    "\n",
    "print(\"Time with copy:\")\n",
    "print(tiempo.gpu_times.mean(), \"±\", tiempo.gpu_times.std())\n",
    "\n",
    "a_gpu = cp.random.rand(size, dtype=cp.float64)\n",
    "b_gpu = cp.random.rand(size, dtype=cp.float64)\n",
    "\n",
    "_ = grade2_ufunc(a_gpu, b_gpu, a, b, c)\n",
    "\n",
    "tiempo = benchmark(\n",
    "    grade2_ufunc,\n",
    "    (a_gpu, b_gpu, a, b, c),\n",
    "    n_repeat=5\n",
    ")\n",
    "\n",
    "print(\"\\nTime without copy:\")\n",
    "print(tiempo.gpu_times.mean(), \"±\", tiempo.gpu_times.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b550a8-30cb-4c96-bc46-a6d6a0c835bb",
   "metadata": {},
   "source": [
    "## GPU con Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d326da8-9bc3-4931-b1e4-0b2962eb259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Time with copy:\n",
      "0.01915316581726074 ± 0.002195424873061129\n",
      "\n",
      "Time without copy:\n",
      "0.0025166034698486327 ± 0.0014511907664684178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(\n",
    "    ['float64(float64, float64, float64, float64, float64)'],\n",
    "    target='cuda'\n",
    ")\n",
    "def grade2_numba_gpu(x, y, a, b, c):\n",
    "    return a*x*x + b*y + c\n",
    "\n",
    "size = 5_000_000\n",
    "\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10.0\n",
    "\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "def compute_with_copy():\n",
    "    return grade2_numba_gpu(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "_ = compute_with_copy()\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "    _ = compute_with_copy()\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "print(\"Time with copy:\")\n",
    "print(np.mean(times), \"±\", np.std(times))\n",
    "\n",
    "a_gpu = cp.asarray(a_cpu)\n",
    "b_gpu = cp.asarray(b_cpu)\n",
    "\n",
    "def compute_no_copy():\n",
    "    return grade2_numba_gpu(a_gpu, b_gpu, a, b, c)\n",
    "\n",
    "_ = compute_no_copy()\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "    _ = compute_no_copy()\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "print(\"\\nTime without copy:\")\n",
    "print(np.mean(times), \"±\", np.std(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6458576-ccc7-4140-a4f4-7780e5b0b8f8",
   "metadata": {},
   "source": [
    "## Analisis de resultados\n",
    "\n",
    "En primer lugar, si se toma como referencia la ejecución en CPU, se aprecia que el uso de Numba en modo secuencial supone una mejora clara frente al código original. Mientras que las versiones basadas en ufuncs de NumPy presentan tiempos cercanos a 0.018 segundos, la versión con Numba reduce el tiempo de ejecución hasta aproximadamente 0.008 segundos, lo que indica que la compilación JIT ya resulta efectiva incluso sin recurrir a la GPU.\n",
    "\n",
    "Por otro lado, al ejecutar el cálculo en GPU mediante la librería CuPy, se observa una diferencia muy marcada entre los casos en los que es necesario copiar los datos desde la CPU y aquellos en los que los datos se generan directamente en la GPU. En el primer caso, cuando existe transferencia de datos entre CPU y GPU, el tiempo total se sitúa en torno a 0.018 segundos, lo que muestra que el coste de dicha copia tiene un impacto notable en el rendimiento global.\n",
    "\n",
    "Sin embargo, cuando los arrays se crean directamente en la GPU y se evita la transferencia de datos, el tiempo de ejecución se reduce considerablemente hasta aproximadamente 0.0019 segundos. De este modo, queda claro que el cálculo en la GPU es muy rápido y que la principal penalización aparece únicamente cuando se incluyen operaciones de copia de memoria.\n",
    "\n",
    "De manera similar, al emplear Numba para ejecutar el cálculo en GPU se obtienen resultados coherentes con los observados en CuPy. Cuando la copia de datos entre CPU y GPU se realiza de forma automática, el tiempo vuelve a situarse alrededor de 0.018 segundos, mientras que, al copiar los datos manualmente fuera de la medición y mantenerlos en la GPU, el tiempo desciende hasta aproximadamente 0.0016 segundos, alcanzando rendimientos prácticamente equivalentes a los de CuPy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
